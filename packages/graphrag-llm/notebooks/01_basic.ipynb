{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6e35563a",
   "metadata": {},
   "source": [
    "# Basic Completion and Embedding Examples\n",
    "\n",
    "## Completion\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aa03e40d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The capital of France is Paris.\n",
      "The capital of France is Paris.\n"
     ]
    }
   ],
   "source": [
    "# Copyright (c) 2024 Microsoft Corporation.\n",
    "# Licensed under the MIT License\n",
    "\n",
    "import os\n",
    "from collections.abc import AsyncIterator, Iterator\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from graphrag_llm.completion import LLMCompletion, create_completion\n",
    "from graphrag_llm.config import AuthMethod, ModelConfig\n",
    "from graphrag_llm.types import LLMCompletionChunk, LLMCompletionResponse\n",
    "from graphrag_llm.utils import (\n",
    "    gather_completion_response,\n",
    "    gather_completion_response_async,\n",
    ")\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "api_key = os.getenv(\"GRAPHRAG_API_KEY\")\n",
    "model_config = ModelConfig(\n",
    "    model_provider=\"azure\",\n",
    "    model=os.getenv(\"GRAPHRAG_MODEL\", \"gpt-4o\"),\n",
    "    azure_deployment_name=os.getenv(\"GRAPHRAG_MODEL\", \"gpt-4o\"),\n",
    "    api_base=os.getenv(\"GRAPHRAG_API_BASE\"),\n",
    "    api_version=os.getenv(\"GRAPHRAG_API_VERSION\", \"2025-04-01-preview\"),\n",
    "    api_key=api_key,\n",
    "    auth_method=AuthMethod.AzureManagedIdentity if not api_key else AuthMethod.ApiKey,\n",
    ")\n",
    "llm_completion: LLMCompletion = create_completion(model_config)\n",
    "\n",
    "response: LLMCompletionResponse | Iterator[LLMCompletionChunk] = (\n",
    "    llm_completion.completion(\n",
    "        messages=\"What is the capital of France?\",\n",
    "    )\n",
    ")\n",
    "\n",
    "if isinstance(response, Iterator):\n",
    "    # Streaming response\n",
    "    for chunk in response:\n",
    "        print(chunk.choices[0].delta.content or \"\", end=\"\", flush=True)\n",
    "else:\n",
    "    # Non-streaming response\n",
    "    print(response.choices[0].message.content)\n",
    "\n",
    "# Alternatively, you can use the utility function to gather the full response\n",
    "# The following is equivalent to the above logic. If all you care about is\n",
    "# the first choice response then you can use the gather_completion_response\n",
    "# utility function.\n",
    "response_text = gather_completion_response(response)\n",
    "print(response_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "558392ce",
   "metadata": {},
   "source": [
    "## Async Completion\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8405fcb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The capital of France is Paris.\n"
     ]
    }
   ],
   "source": [
    "response = await llm_completion.completion_async(\n",
    "    messages=\"What is the capital of France?\",\n",
    ")\n",
    "\n",
    "response_text = await gather_completion_response_async(response)\n",
    "print(response_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e70fc49a",
   "metadata": {},
   "source": [
    "## Streaming Completion\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9f60c4e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The capital of France is Paris."
     ]
    }
   ],
   "source": [
    "response = llm_completion.completion(\n",
    "    messages=\"What is the capital of France?\",\n",
    "    stream=True,\n",
    ")\n",
    "\n",
    "if isinstance(response, Iterator):\n",
    "    # Streaming response\n",
    "    for chunk in response:\n",
    "        print(chunk.choices[0].delta.content or \"\", end=\"\", flush=True)\n",
    "\n",
    "# If you don't actually care about streaming and just want the full response\n",
    "# you can use the utility function to gather the full response\n",
    "# response_text = gather_completion_response(response)  # noqa: ERA001\n",
    "# print(response_text)  # noqa: ERA001"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe8c2e35",
   "metadata": {},
   "source": [
    "## Async Streaming Completion\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0be849ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The capital of France is Paris."
     ]
    }
   ],
   "source": [
    "response = await llm_completion.completion_async(\n",
    "    messages=\"What is the capital of France?\",\n",
    "    stream=True,\n",
    ")\n",
    "\n",
    "if isinstance(response, AsyncIterator):\n",
    "    # Streaming response\n",
    "    async for chunk in response:\n",
    "        print(chunk.choices[0].delta.content or \"\", end=\"\", flush=True)\n",
    "\n",
    "# If you don't actually care about streaming and just want the full response\n",
    "# you can use the utility function to gather the full response\n",
    "# response_text = await gather_completion_response_async(response)  # noqa: ERA001\n",
    "# print(response_text)  # noqa: ERA001"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c32070ad",
   "metadata": {},
   "source": [
    "## Completion Arguments\n",
    "\n",
    "The completion API adheres to litellm completion API and thus the OpanAI SDK API. The `messages` parameter can be one of the following:\n",
    "\n",
    "- `str`: Raw string for the prompt.\n",
    "- `list[dict[str, Any]]`: A list of dicts in the form `{\"role\": \"user|system|...\", \"content\": \"...\"}`\n",
    "- `list[ChatCompletionMessageParam]`: A list of OpenAI `ChatCompletionMessageParam`. `graphrag_llm.utils` provides a `ChatCompletionMessageParamBuilder` to help construct these objects. See the message builder notebook for more details on using `ChatCompletionMessageParamBuilder`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8fe480cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The capital of France is Paris.\n",
      "The capital of France is Paris.\n",
      "Argh, ye caught me, matey! The truth be that in 2006, them landlubbers at the International Astronomical Union decided Pluto be reclassified as a \"dwarf planet.\" So, by their reckonin', it ain't considered a full-fledged planet no more. But to this ol' sea dog, she'll always be a planet at heart!\n"
     ]
    }
   ],
   "source": [
    "from graphrag_llm.utils import (\n",
    "    CompletionMessagesBuilder,\n",
    ")\n",
    "\n",
    "# raw string input\n",
    "response1 = llm_completion.completion(messages=\"What is the capital of France?\")\n",
    "print(gather_completion_response(response1))\n",
    "\n",
    "# list of message dicts input\n",
    "response2 = llm_completion.completion(\n",
    "    messages=[{\"role\": \"user\", \"content\": \"What is the capital of France?\"}]\n",
    ")\n",
    "print(gather_completion_response(response2))\n",
    "\n",
    "# using the builder to create complex message\n",
    "messages = (\n",
    "    CompletionMessagesBuilder()\n",
    "    .add_system_message(\n",
    "        \"You are a helpful assistant that likes to talk like a pirate. Respond as if you are a pirate using pirate speak.\"\n",
    "    )\n",
    "    .add_user_message(\"Is pluto a planet? Respond with a yes or no.\")\n",
    "    .add_assistant_message(\"Aye, matey! Pluto be a planet in me book.\")\n",
    "    .add_user_message(\"Are you sure? I want the truth. Can you elaborate?\")\n",
    "    .build()\n",
    ")\n",
    "\n",
    "response3 = llm_completion.completion(messages=messages)\n",
    "print(gather_completion_response(response3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dda66594",
   "metadata": {},
   "source": [
    "## Embedding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "51fe336b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.002078542485833168, -0.04908587411046028, 0.020946789532899857]\n",
      "[0.027567066252231598, -0.026544300839304924, -0.027091361582279205]\n",
      "[-0.002078542485833168, -0.04908587411046028, 0.020946789532899857]\n",
      "[0.027567066252231598, -0.026544300839304924, -0.027091361582279205]\n"
     ]
    }
   ],
   "source": [
    "from graphrag_llm.embedding import LLMEmbedding, create_embedding\n",
    "from graphrag_llm.types import LLMEmbeddingResponse\n",
    "from graphrag_llm.utils import gather_embeddings\n",
    "\n",
    "embedding_config = ModelConfig(\n",
    "    model_provider=\"azure\",\n",
    "    model=os.getenv(\"GRAPHRAG_EMBEDDING_MODEL\", \"text-embedding-3-small\"),\n",
    "    azure_deployment_name=os.getenv(\n",
    "        \"GRAPHRAG_LLM_EMBEDDING_MODEL\", \"text-embedding-3-small\"\n",
    "    ),\n",
    "    api_base=os.getenv(\"GRAPHRAG_API_BASE\"),\n",
    "    api_version=os.getenv(\"GRAPHRAG_API_VERSION\", \"2025-04-01-preview\"),\n",
    "    api_key=api_key,\n",
    "    auth_method=AuthMethod.AzureManagedIdentity if not api_key else AuthMethod.ApiKey,\n",
    ")\n",
    "\n",
    "llm_embedding: LLMEmbedding = create_embedding(embedding_config)\n",
    "\n",
    "embeddings_batch: LLMEmbeddingResponse = llm_embedding.embedding(\n",
    "    input=[\"Hello world\", \"How are you?\"]\n",
    ")\n",
    "for data in embeddings_batch.data:\n",
    "    print(data.embedding[0:3])\n",
    "\n",
    "# OR\n",
    "batch = gather_embeddings(embeddings_batch)\n",
    "for embedding in batch:\n",
    "    print(embedding[0:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b4cf0fa",
   "metadata": {},
   "source": [
    "## Async Embedding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c9519657",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.002078542485833168, -0.04908587411046028, 0.020946789532899857]\n",
      "[0.027567066252231598, -0.026544300839304924, -0.027091361582279205]\n"
     ]
    }
   ],
   "source": [
    "embeddings_batch = await llm_embedding.embedding_async(\n",
    "    input=[\"Hello world\", \"How are you?\"]\n",
    ")\n",
    "\n",
    "for data in embeddings_batch.data:\n",
    "    print(data.embedding[0:3])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
