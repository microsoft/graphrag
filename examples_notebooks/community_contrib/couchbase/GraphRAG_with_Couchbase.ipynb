{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fbw833Y2gckr"
   },
   "source": [
    "# Tutorial on Graph RAG(Local Search) with Couchbase Vector Store\n",
    "This notebook walks through the process of setting up a search engine that combines Couchbase for storing embeddings, OpenAI's models for generating embeddings, knowledge graph and communities from textual data.\n",
    "\n",
    "## Setting up Couchbase\n",
    "\n",
    "Before running this notebook, set up the following in Couchbase:\n",
    "\n",
    "1. Create a bucket named \"graphrag-demo\" (or as specified in COUCHBASE_BUCKET_NAME)\n",
    "2. Within the bucket, create a scope named \"shared\" (or as specified in COUCHBASE_SCOPE_NAME)\n",
    "3. Within the scope, create a collection named \"entity_description_embeddings\" (or as specified in COUCHBASE_COLLECTION_NAME)\n",
    "\n",
    "These settings should match the environment variables defined in your .env file or the default values in the code.\n",
    "\n",
    "4. In the Couchbase Full Text Search (FTS) index section, create a new index by importing the `graphrag_demo_index.json` file. This file contains the necessary configuration for the vector search index.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Local and Global Search in Graph RAG Systems\n",
    "\n",
    "Local and global search are two approaches used in Graph RAG (Retrieval-Augmented Generation) systems:\n",
    "\n",
    "### Local Search\n",
    "Local search method generates answers by combining relevant data from the AI-extracted knowledge-graph with text chunks of the raw documents. This method is suitable for questions that require an understanding of specific entities mentioned in the documents.\n",
    "\n",
    "### Global Search\n",
    "Global search method generates answers by searching over all AI-generated community reports in a map-reduce fashion. This is a resource-intensive method, but often gives good responses for questions that require an understanding of the dataset as a whole.\n",
    "\n",
    "## Couchbase as a Vector Store for Local Search\n",
    "\n",
    "Couchbase can be used as a vector store to support local search in Graph RAG systems. Its capabilities include:\n",
    "\n",
    "- **Vector Storage**: Couchbase can store vector embeddings alongside document data.\n",
    "- **Vector Search**: It supports similarity search on vector fields using algorithms like cosine similarity.\n",
    "- **Indexing**: Couchbase offers indexing options to optimize vector searches.\n",
    "- **Scalability**: As a distributed database, it can handle large amounts of vector data.\n",
    "\n",
    "To implement local search, you would store node embeddings in Couchbase and use its vector search capabilities to find similar nodes efficiently within a local context.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PaN5siuBgctS"
   },
   "source": [
    "# Importing Necessary Libraries\n",
    "\n",
    "In this section, we import all the essential Python libraries required to perform various tasks, \n",
    "such as loading data, interacting with Couchbase, and using OpenAI models for generating text and embeddings.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "5tIHss5Rglye"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "import tiktoken\n",
    "from couchbase.auth import PasswordAuthenticator\n",
    "from couchbase.options import ClusterOptions\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from graphrag.query.context_builder.entity_extraction import EntityVectorStoreKey\n",
    "from graphrag.query.indexer_adapters import (\n",
    "    read_indexer_covariates,\n",
    "    read_indexer_entities,\n",
    "    read_indexer_relationships,\n",
    "    read_indexer_reports,\n",
    "    read_indexer_text_units,\n",
    ")\n",
    "from graphrag.query.input.loaders.dfs import store_entity_semantic_embeddings\n",
    "from graphrag.query.llm.oai.chat_openai import ChatOpenAI\n",
    "from graphrag.query.llm.oai.embedding import OpenAIEmbedding\n",
    "from graphrag.query.llm.oai.typing import OpenaiApiType\n",
    "from graphrag.query.structured_search.local_search.mixed_context import (\n",
    "    LocalSearchMixedContext,\n",
    ")\n",
    "from graphrag.query.structured_search.local_search.search import LocalSearch\n",
    "from graphrag.vector_stores.couchbasedb import CouchbaseVectorStore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2efKWBqpgcw-"
   },
   "source": [
    "# Configuring Environment Variables\n",
    "Here, we configure various environment variables that define paths, API keys, and connection strings. These values are essential for connecting to Couchbase and OpenAI, loading data, and defining other constants.\n",
    "\n",
    "- INPUT_DIR: This specifies the directory path where the input data files are located. These files typically contain the raw data that will be processed and analyzed in the notebook.\n",
    "- COUCHBASE_CONNECTION_STRING: This is the connection string used to establish a connection with the Couchbase database. It usually includes the protocol and host information (e.g., \"couchbase://localhost\").\n",
    "- OPENAI_API_KEY: This is your personal API key for accessing OpenAI's services. It's required for authentication when making requests to OpenAI's API, allowing you to use their language models and other AI services.\n",
    "- LLM_MODEL: This variable specifies which Large Language Model (LLM) from OpenAI to use for text generation tasks. For example, it could be set to \"gpt-4\" for using GPT-4, or \"gpt-3.5-turbo\" for using ChatGPT.\n",
    "- EMBEDDING_MODEL: This defines the specific model used for generating text embeddings. Text embeddings are vector representations of text that capture semantic meaning. For OpenAI, a common choice is \"text-embedding-ada-002\".\n",
    "\n",
    "These environment variables are crucial for the notebook's functionality, as they provide necessary configuration details for data access, database connections, and AI model interactions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "Cz1PfM7zgc39"
   },
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "\n",
    "INPUT_DIR = os.getenv(\"INPUT_DIR\")\n",
    "COUCHBASE_CONNECTION_STRING = os.getenv(\n",
    "    \"COUCHBASE_CONNECTION_STRING\", \"couchbase://localhost\"\n",
    ")\n",
    "COUCHBASE_USERNAME = os.getenv(\"COUCHBASE_USERNAME\", \"Administrator\")\n",
    "COUCHBASE_PASSWORD = os.getenv(\"COUCHBASE_PASSWORD\", \"password\")\n",
    "COUCHBASE_BUCKET_NAME = os.getenv(\"COUCHBASE_BUCKET_NAME\", \"graphrag-demo\")\n",
    "COUCHBASE_SCOPE_NAME = os.getenv(\"COUCHBASE_SCOPE_NAME\", \"shared\")\n",
    "COUCHBASE_COLLECTION_NAME = os.getenv(\n",
    "    \"COUCHBASE_COLLECTION_NAME\", \"entity_description_embeddings\"\n",
    ")\n",
    "COUCHBASE_VECTOR_INDEX_NAME = os.getenv(\"COUCHBASE_VECTOR_INDEX_NAME\", \"graphrag_index\")\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "LLM_MODEL = os.getenv(\"LLM_MODEL\", \"gpt-4o\")\n",
    "EMBEDDING_MODEL = os.getenv(\"EMBEDDING_MODEL\", \"text-embedding-ada-002\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VlnoR17Tgc7b"
   },
   "source": [
    "## Load text units and graph data tables as context for local search\n",
    "In this part, we load data from Parquet files into a dictionary.We define functions that will handle the loading and processing of each paraquet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {}\n",
    "\n",
    "# Constants\n",
    "COMMUNITY_LEVEL = 2\n",
    "\n",
    "# Table names\n",
    "TABLE_NAMES = {\n",
    "    \"COMMUNITY_REPORT_TABLE\": \"create_final_community_reports\",\n",
    "    \"ENTITY_TABLE\": \"create_final_nodes\",\n",
    "    \"ENTITY_EMBEDDING_TABLE\": \"create_final_entities\",\n",
    "    \"RELATIONSHIP_TABLE\": \"create_final_relationships\",\n",
    "    \"COVARIATE_TABLE\": \"create_final_covariates\",\n",
    "    \"TEXT_UNIT_TABLE\": \"create_final_text_units\",\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read Entities:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    data[\"entities\"] = pd.read_parquet(\n",
    "        f\"{INPUT_DIR}/{TABLE_NAMES['ENTITY_TABLE']}.parquet\"\n",
    "    )\n",
    "    entity_embeddings = pd.read_parquet(\n",
    "        f\"{INPUT_DIR}/{TABLE_NAMES['ENTITY_EMBEDDING_TABLE']}.parquet\"\n",
    "    )\n",
    "    data[\"entities\"] = read_indexer_entities(\n",
    "        data[\"entities\"], entity_embeddings, COMMUNITY_LEVEL\n",
    "    )\n",
    "except FileNotFoundError:\n",
    "    data[\"entities\"] = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read Relationships:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    data[\"relationships\"] = pd.read_parquet(\n",
    "        f\"{INPUT_DIR}/{TABLE_NAMES['RELATIONSHIP_TABLE']}.parquet\"\n",
    "    )\n",
    "    data[\"relationships\"] = read_indexer_relationships(data[\"relationships\"])\n",
    "except FileNotFoundError:\n",
    "    data[\"relationships\"] = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read Covariates:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    data[\"covariates\"] = pd.read_parquet(\n",
    "        f\"{INPUT_DIR}/{TABLE_NAMES['COVARIATE_TABLE']}.parquet\"\n",
    "    )\n",
    "    data[\"covariates\"] = read_indexer_covariates(data[\"covariates\"])\n",
    "except FileNotFoundError:\n",
    "    data[\"covariates\"] = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read Reports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    data[\"reports\"] = pd.read_parquet(\n",
    "        f\"{INPUT_DIR}/{TABLE_NAMES['COMMUNITY_REPORT_TABLE']}.parquet\"\n",
    "    )\n",
    "    entity_data = pd.read_parquet(f\"{INPUT_DIR}/{TABLE_NAMES['ENTITY_TABLE']}.parquet\")\n",
    "    data[\"reports\"] = read_indexer_reports(\n",
    "        data[\"reports\"], entity_data, COMMUNITY_LEVEL\n",
    "    )\n",
    "except FileNotFoundError:\n",
    "    data[\"reports\"] = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read Text units:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loading completed\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    data[\"text_units\"] = pd.read_parquet(\n",
    "        f\"{INPUT_DIR}/{TABLE_NAMES['TEXT_UNIT_TABLE']}.parquet\"\n",
    "    )\n",
    "    data[\"text_units\"] = read_indexer_text_units(data[\"text_units\"])\n",
    "except FileNotFoundError:\n",
    "    data[\"text_units\"] = None\n",
    "\n",
    "print(\"Data loading completed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L8AOUrIAgc-8"
   },
   "source": [
    "# Setting Up the Couchbase Vector Store\n",
    "Couchbase is used here to store the semantic embeddings generated from entities. In this step, we define a method to connect to the Couchbase database using the provided credentials.\n",
    "\n",
    "The CouchbaseVectorStore allows you to store, retrieve, and manage vector embeddings in Couchbase.\n",
    "The connect() method initializes the connection to Couchbase using the provided connection string, username, and password."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "kiYpzj7-gdC4"
   },
   "outputs": [],
   "source": [
    "couchbase_vector_store = CouchbaseVectorStore(\n",
    "    collection_name=COUCHBASE_COLLECTION_NAME,\n",
    "    bucket_name=COUCHBASE_BUCKET_NAME,\n",
    "    scope_name=COUCHBASE_SCOPE_NAME,\n",
    "    index_name=COUCHBASE_VECTOR_INDEX_NAME,\n",
    ")\n",
    "\n",
    "auth = PasswordAuthenticator(str(COUCHBASE_USERNAME), str(COUCHBASE_PASSWORD))\n",
    "cluster_options = ClusterOptions(auth)\n",
    "\n",
    "couchbase_vector_store.connect(\n",
    "    connection_string=COUCHBASE_CONNECTION_STRING,\n",
    "    cluster_options=cluster_options,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YJZhrj5egdGt"
   },
   "source": [
    "# Setting Up Language Models\n",
    "In this section, we configure the language models using OpenAI’s API. We initialize:\n",
    "\n",
    "ChatOpenAI: This is the language model used to generate responses to natural language queries.\n",
    "OpenAIEmbedding: This is the model used to generate vector embeddings for text data.\n",
    "tiktoken: This tokenizer is used to split text into tokens, which are essential for sending data to the language model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "japySJrUgdOG"
   },
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(\n",
    "    api_key=OPENAI_API_KEY,\n",
    "    model=LLM_MODEL,\n",
    "    api_type=OpenaiApiType.OpenAI,\n",
    "    max_retries=20,\n",
    ")\n",
    "\n",
    "token_encoder = tiktoken.get_encoding(\"cl100k_base\")\n",
    "\n",
    "text_embedder = OpenAIEmbedding(\n",
    "    api_key=OPENAI_API_KEY,\n",
    "    api_base=None,\n",
    "    api_type=OpenaiApiType.OpenAI,\n",
    "    model=EMBEDDING_MODEL,\n",
    "    deployment_name=EMBEDDING_MODEL,\n",
    "    max_retries=20,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r08V2NuugdSF"
   },
   "source": [
    "# Storing Embeddings in Couchbase\n",
    "After generating embeddings for the entities, we store them in Couchbase. We use the store_entity_semantic_embeddings function to store the embeddings.\n",
    "\n",
    "This method checks if the input is either a dictionary or a list and processes it accordingly.\n",
    "It uses the Couchbase vector store to save the embeddings, ensuring that entities have the proper 'id' attribute for storage.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "C1wV0RqrgdVl"
   },
   "outputs": [],
   "source": [
    "\n",
    "try:\n",
    "    if not isinstance(data[\"entities\"], list):\n",
    "        error_message = \"data['entities'] must be a list\"\n",
    "        raise TypeError(error_message)\n",
    "\n",
    "    store_entity_semantic_embeddings(\n",
    "        entities=data[\"entities\"], vectorstore=couchbase_vector_store\n",
    "    )\n",
    "except AttributeError as err:\n",
    "    error_message = \"Error storing entity semantic embeddings. Ensure all entities have an 'id' attribute\"\n",
    "    raise AttributeError(error_message) from err\n",
    "except TypeError as err:\n",
    "    error_message = \"Error storing entity semantic embeddings. Ensure data['entities'] is a list\"\n",
    "    raise TypeError(error_message) from err\n",
    "except Exception as err:\n",
    "    error_message = \"Error storing entity semantic embeddings\"\n",
    "    raise Exception(error_message) from err\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f0LSSSJlgdZd"
   },
   "source": [
    "### **7. Building the Search Engine (In the Context of Graphrag)**\n",
    "\n",
    "Here, we explain the components of the search engine in detail and how they contribute to its functionality within Graphrag.\n",
    "\n",
    "#### **1. Context Builder (LocalSearchMixedContext)**\n",
    "\n",
    "The `LocalSearchMixedContext` class is the cornerstone of our search engine in Graphrag. It acts as a **contextual environment** for the search process by combining various types of data—such as **community reports, text units, entities, relationships, and covariates**—into a coherent structure that can be used by the search engine. In this context:\n",
    "\n",
    "- **Community Reports**: These are structured documents or insights generated at a community level, such as summaries or analytics reports, which are crucial when trying to query community-specific data.\n",
    "- **Text Units**: Smaller pieces of text, such as paragraphs, sentences, or tokens that are stored in the system. These units help in understanding specific parts of the context when answering questions.\n",
    "- **Entities**: These represent the core subjects (people, organizations, products, etc.) around which your queries are structured. Each entity has certain attributes and semantic embeddings stored in Couchbase, and these are used to enrich the search results.\n",
    "- **Relationships**: The connections between entities, which can represent anything from business partnerships to familial ties or data dependencies. Understanding these relationships helps in contextualizing the search results more effectively.\n",
    "- **Covariates**: Additional variables or metadata that provide more information about entities and relationships. These could include factors like location, time, or other metrics that affect the relevance of the search.\n",
    "\n",
    "All these elements work together to build the **context** that the search engine will use to find and rank results.\n",
    "\n",
    "- **entity_text_embeddings**: The entity descriptions are stored as vector embeddings (using the Couchbase vector store) to help in finding semantically similar entities.\n",
    "- **text_embedder**: This is the **OpenAI embedding model** used to embed both the entities and user queries in a similar vector space, allowing for meaningful similarity comparisons.\n",
    "- **token_encoder**: Tokenization splits the input text into tokens (smaller chunks), making it easier to process by the language models.\n",
    "\n",
    "#### **2. Local Search Parameters**\n",
    "\n",
    "Once the context is established, we define the parameters for the **search engine**. These parameters guide how the search engine processes the context to answer a query.\n",
    "\n",
    "- **text_unit_prop**: This sets the proportion of text units to be considered when building the context. In this case, 50% of the context comes from text units.\n",
    "- **community_prop**: Similar to `text_unit_prop`, this defines how much weight to give community reports. Here, 10% of the context is derived from community reports.\n",
    "- **conversation_history_max_turns**: This specifies how many conversation history turns are retained when building the context. It helps in multi-turn queries, where the context from previous queries may still be relevant.\n",
    "- **top_k_mapped_entities**: Defines how many of the most relevant entities should be considered in each query. In this case, we are considering the top 10 entities.\n",
    "- **top_k_relationships**: Similarly, we consider the top 10 relationships that are most relevant to the query.\n",
    "- **include_entity_rank**: Whether to rank entities based on their relevance to the query.\n",
    "- **include_relationship_weight**: Whether to include relationship weights in the ranking process. This is crucial because certain relationships may have higher importance based on the data being queried.\n",
    "- **embedding_vectorstore_key**: Defines the **key** for accessing entity embeddings from Couchbase. Here, we use `EntityVectorStoreKey.ID` as the identifier for retrieving the correct embeddings.\n",
    "- **max_tokens**: The maximum number of tokens to consider in the context.\n",
    "\n",
    "\n",
    "#### **3. Language Model Parameters**\n",
    "\n",
    "For answering the query, we use the **language model** (LLM) to generate the response. The parameters for the LLM are configured here:\n",
    "- **max_tokens**: Limits the number of tokens (words or sub-words) in the generated answer.\n",
    "- **temperature**: Controls the randomness of the output. Setting it to `0.0` makes the model’s answers more deterministic.\n",
    "\n",
    "\n",
    "#### **4. Integrating Everything: Creating the Search Engine**\n",
    "\n",
    "Finally, all components are integrated into the `LocalSearch` class, which serves as the main search engine. This class is responsible for:\n",
    "- **Accepting queries** in natural language.\n",
    "- **Using the context builder** to form a detailed context based on the available structured data (entities, relationships, text, reports).\n",
    "- **Passing the query and context** to the language model (LLM), which generates the final answer.\n",
    "\n",
    "The search engine is now ready to process queries, using the underlying Graphrag system to provide context-aware and semantically rich answers.\n",
    "\n",
    "\n",
    "### **Summary**\n",
    "\n",
    "This search engine leverages **structured data** (entities, relationships, reports, etc.) generated from the input files and integrates **semantic embeddings** stored in Couchbase. The search engine processes the query using OpenAI's language model, which uses the structured data context of the graph RAG to generate meaningful answers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "8ZiML072gddQ"
   },
   "outputs": [],
   "source": [
    "context_builder = LocalSearchMixedContext(\n",
    "    community_reports=data[\"reports\"],\n",
    "    text_units=data[\"text_units\"],\n",
    "    entities=data[\"entities\"],\n",
    "    relationships=data[\"relationships\"],\n",
    "    covariates=data[\"covariates\"],\n",
    "    entity_text_embeddings=couchbase_vector_store,\n",
    "    embedding_vectorstore_key=EntityVectorStoreKey.ID,\n",
    "    text_embedder=text_embedder,\n",
    "    token_encoder=token_encoder,\n",
    ")\n",
    "\n",
    "local_context_params = {\n",
    "    \"text_unit_prop\": 0.5,\n",
    "    \"community_prop\": 0.1,\n",
    "    \"conversation_history_max_turns\": 5,\n",
    "    \"conversation_history_user_turns_only\": True,\n",
    "    \"top_k_mapped_entities\": 10,\n",
    "    \"top_k_relationships\": 10,\n",
    "    \"include_entity_rank\": True,\n",
    "    \"include_relationship_weight\": True,\n",
    "    \"include_community_rank\": False,\n",
    "    \"return_candidate_context\": False,\n",
    "    \"embedding_vectorstore_key\": EntityVectorStoreKey.ID,\n",
    "    \"max_tokens\": 12_000,\n",
    "}\n",
    "\n",
    "llm_params = {\n",
    "    \"max_tokens\": 2_000,\n",
    "    \"temperature\": 0.0,\n",
    "}\n",
    "\n",
    "search_engine = LocalSearch(\n",
    "    llm=llm,\n",
    "    context_builder=context_builder,\n",
    "    token_encoder=token_encoder,\n",
    "    llm_params=llm_params,\n",
    "    context_builder_params=local_context_params,\n",
    "    response_type=\"multiple paragraphs\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "By8DVv-Igdg0"
   },
   "source": [
    "# Running a Query\n",
    "Finally, we run a query on the search engine. In this case, the query is \"Give me a summary about the story\". This simulates asking the search engine to summarize the entities and relationships stored in Couchbase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "1SvUSrIbgdkh"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: 'Give me a summary about the story'\n",
      "Answer: # Summary of the Story\n",
      "\n",
      "## Introduction to the Paranormal Military Squad\n",
      "\n",
      "The narrative centers around the Paranormal Military Squad, a secretive governmental faction tasked with investigating and engaging with extraterrestrial phenomena. This elite group operates primarily from the Dulce military base, where they are deeply involved in Operation: Dulce. The mission's primary objective is to mediate Earth's contact with alien intelligence, ensuring humanity's safety and preparing for potential first contact scenarios [Data: Paranormal Military Squad and Operation: Dulce (18)].\n",
      "\n",
      "## Key Figures and Their Roles\n",
      "\n",
      "Key figures within the squad include Alex Mercer, Dr. Jordan Hayes, Taylor Cruz, and Sam Rivera. Alex Mercer provides leadership and strategic insights, guiding the team through high-stakes operations. Dr. Jordan Hayes focuses on deciphering alien codes and understanding their intent, contributing significantly to the team's mission. Taylor Cruz oversees the team's efforts, providing strategic insights and emphasizing diligence. Sam Rivera brings youthful vigor and optimism, handling technical tasks, particularly in communications and signal interpretation [Data: Paranormal Military Squad and Operation: Dulce (18); Entities (30, 31, 38, 94)].\n",
      "\n",
      "## Operation: Dulce\n",
      "\n",
      "Operation: Dulce is a significant mission undertaken by the Paranormal Military Squad, focusing on mediating Earth's contact with alien intelligence. This operation involves investigating cosmic phenomena, decrypting alien communications, and preparing for potential first contact scenarios. The Dulce military base is equipped with high-tech equipment specifically designed for decoding alien communications, making it a strategic location for the squad's activities [Data: Paranormal Military Squad and Operation: Dulce (18); Relationships (142, 143, 144, 194, 196)].\n",
      "\n",
      "## Deciphering Alien Signals\n",
      "\n",
      "A primary focus of the Paranormal Military Squad is deciphering alien signals. This involves analyzing and decoding extraterrestrial communications to understand their intent and ensure humanity's safety. The squad's efforts in this area are critical for preparing for potential first contact scenarios and establishing effective communication with alien races. The use of advanced communications equipment and the strategic location at the Dulce military base are essential components of this mission [Data: Paranormal Military Squad and Operation: Dulce (18); Relationships (125, 126, 134, 108, 114, +more)].\n",
      "\n",
      "## Potential First Contact Scenarios\n",
      "\n",
      "The Paranormal Military Squad is preparing for potential first contact scenarios with extraterrestrial intelligence. This involves mediating Earth's bid for cosmic relevance through dialogue, ensuring effective communication and negotiation with alien beings. The squad's role in these scenarios is critical, as they represent humanity in these unprecedented encounters. The potential implications of first contact are monumental, making the squad's mission highly significant [Data: Paranormal Military Squad and Operation: Dulce (18); Relationships (119, 118, 109, 107, 127, +more)].\n",
      "\n",
      "## Global Implications of the Mission\n",
      "\n",
      "The activities of the Paranormal Military Squad have significant global implications. Their mission to engage with extraterrestrial intelligence and prepare for potential first contact scenarios could alter the course of human history. The squad's efforts in deciphering alien signals, ensuring humanity's safety, and establishing effective communication with alien races are critical for navigating the complexities of cosmic discovery. The potential for both positive and negative outcomes makes the squad's mission highly impactful [Data: Paranormal Military Squad and Operation: Dulce (18); Relationships (110, 111, 140, 123, 135, +more)].\n",
      "\n",
      "## Conclusion\n",
      "\n",
      "In summary, the story of the Paranormal Military Squad and Operation: Dulce is a gripping tale of humanity's quest to understand and engage with extraterrestrial intelligence. The squad's mission is fraught with challenges and potential dangers, but their work is crucial for the future of human civilization. Through their efforts, they aim to bridge the gap between Earth and the cosmos, ensuring that humanity is prepared for whatever lies beyond the stars.\n"
     ]
    }
   ],
   "source": [
    "question = \"Give me a summary about the story\"\n",
    "\n",
    "try:\n",
    "    result = await search_engine.asearch(question)\n",
    "    print(f\"Question: '{question}'\")\n",
    "    print(f\"Answer: {result.response}\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred while processing the query: {(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Wb4weYVBgdn_"
   },
   "source": [
    "With these steps, the entire process of loading data, setting up models, storing embeddings, and running a search engine query is written out in sequence without using functions. Let me know if any additional modifications are needed!"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
